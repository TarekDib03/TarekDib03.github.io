pred3 <- predict(tree3, newdata, type="class")
pred1
pred2
pred3
s$DriveTrain == "Front"
length(s$DriveTrain == "Front")
s1 = s[s$DriveTrain == "Front",]
s1
length(s1)
nrow(s1)
8/12
s <- Cars93[Type=="Large" | Price==20,]
s.front = s[s$DriveTrain=="Front",]
nrow(s.front)/nrow(s)
library(ElemStatLearn)
library(randomForest)
library(e1071)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
vowel.rf <- randomForest(y ~ ., data=vowel.train)
vowel.svm <- svm(y ~ ., data=vowel.train)
pred.rf <- predict(vowel.rf, vowel.test[,-1])
pred.svm <- predict(vowel.svm, vowel.test[,-1])
# 5) What are the error rates for the two approaches on the test data set?
err.rf <- sum(pred.rf != vowel.test$y)/length(pred.rf)
err.svm <- sum(pred.svm != vowel.test$y)/length(pred.svm)
table(vowel.test$y, pred.rf)
table(vowel.test$y, pred.svm)
# 6) What is the error rate when the two methods agree on a prediction?
agree.idx <- which(pred.rf == pred.svm)
err.both <- sum(pred.rf[agree.idx] != vowel.test$y[agree.idx])/length(agree.idx)
err.both
err.svm
agree.idx
err.rf
err.svm
?subset
vowel.train$y
vowel.test
head(vowel.test[,-1])
?svm
err.rf <- sum(pred.rf != vowel.test$y)/length(pred.rf)
err.rf
err.svm <- sum(pred.svm != vowel.test$y)/length(pred.svm)
err.svm
table(vowel.test$y, pred.rf)
table(vowel.test$y, pred.svm)
library(zoo)
library(tseries)
goog <- get.hist.quote(instrument="GOOG", start="2007-07-01", end="2013-06-30", quote="AdjClose",
provider="yahoo")
goog
goog <- get.hist.quote(instrument="si", start="2007-07-01", end="2013-06-30", quote="AdjClose",
provider="yahoo")
goog <- get.hist.quote(instrument="goog", start="2007-07-01", end="2013-06-30", quote="AdjClose",
provider="yahoo")
SI <- get.hist.quote(instrument="SI", start="2007-07-01", end="2013-06-30", quote="AdjClose",
provider="yahoo")
IBM <- get.hist.quote(instrument="IBM", start="2007-07-01", end="2013-06-30", quote="AdjClose",
provider="yahoo")
stocks <- data.frame(goog, SI, IBM)
head(stocks)
105*0.05
(100-99.75)/100
10^-0.1
10^0.1
10^-0.1-1
10^0.1-1
0.5*(-0.2056718)+0.5*0.2589254
log10(1+0.5*(-0.2056718)+0.5*0.2589254)
log(1+0.5*(-0.2056718)+0.5*0.2589254)
log10(1+0.5*(-0.2056718)+0.5*0.2589254)
exp(-1)
exp(-1)-1
exp(1)-1
exp(0.1)-1
0.5*(-0.6321206)+0.5*0.1051709
log(1+0.5*(-0.6321206)+0.5*0.1051709)
log(1-0.2634749)
exp(-0.1)
exp(-0.1)-1
exp(0.1)
exp(0.1)-1
(exp(-0.1)-1)*0.5 + (exp(0.1)-1)*0.5
100/(exp(0.1))
library(Sweave)
install.packages("Sweave")
install.packages("Sweave")
library(tools)
Sweave("example.Snw")
Sweave("example.Snw")
library(MASS)
?cats
ls(package:lattice)
ls(getNamespace("lattice"), all.names=TRUE)
lset()
ls(lset,package:lattice)
ls("lset",package:lattice)
ls(package:lattice)
help("Sweave", package="utils")
rnwfile <- system.file("Sweave", "example-1.Rnw", package = "utils")
Sweave(rnwfile)
pdflatex("example-1.pdf")
(pdf)latex("example-1.pdf")
getwd()
(pdf)latex("example-1.tex")
pdflatex("example-1.tex")
(pdf)latex("example-1.tex")
(pdf)latex('example-1.tex')
(pdf)latex('example-1')
(pdf)latex(example-1.tex)
(pdf)latex(example-1.pdf)
source("Sweave-test-1.R")
source("http://www.openintro.org/stat/data/present.R")
10/11
0.078/0.138
(0.078/0.138)*100
0.18-0.044
0.191-0.036
0.175+0.018
0.191+0.18+0.028
0.18+0.028
0.191+0.18-0.014
if(!require(installr)) {
install.packages("installr"); require(installr)}
updateR(to_checkMD5sums = FALSE)
install.packages()
install.packages()
update.packages(lib.loc = "/usr/local/lib/R/site-library")
install.packages()
install.packages("gplots")
sudo apt-get update
install.packages("ROCR")
install.packages()
version
qt(-0.66646,39)
pt(-0.66646,39)
1-pt(-0.66646,39)
0.09*0.9 + 0.91*0.02
0.02*0.91
0.0992/(0.0992+0.0182)
0.0992/(0.0992+0.1*0.09)
sudo apt-get update
install.packages("foo", dependencies=...)
install.packages("foo", dependencies=T)
install.packages("ROCR", dependencies=T)
install.packages("gplots", dependencies=T)
install.packages("gtools")
install.packages("gdata")
install.packages("gplots", repos = NULL, type="source")
install.packages("ROCR")
library(ROCR)
11/198
1069/1075
update.packages()
install.packages()
install.packages()
install.packages("Rcpp")
install.packages("Rcpp")
library("Rcpp")
install.packages("mice")
library(mice)
library(Rcpp)
library(mice)
qnorm(0.9)
(qnorm(0.9))^2*18^2/16
qt(0.975,499)
qt(0.975,149)
pnorm(-1.2)
pnorm(0.5)
qnorm(0.25)
qnorm(-0.25)
qnorm(0.75)
qnorm(0.995)
0.18-0.15
library(VIM)
vmGUImenu()
library(Rcpp)
library(randomForest)
library(lattice)
library(rpart)
install.packages("mice")
chooseCRANmirror()
install.packages("mice")
library("mice")
2413-460
10*exp(0.06*3)
15*29
30*29/2
2^10
2^30
15*31+1
library(openintro)
data(ncbirths)
births <- data(ncbirths)
summary(births)
data(ncbirths)
ncbirths
summary(ncbirths)
gained_clean <- na.omit(nc$gained)
gained_clean <- na.omit(ncbirths$gained)
n <- length(gained_clean)
n
gained_clean
boot_means <- rep(NA, 100)
for (i in 1:100){
boot_sample <- sample(gained_clean, n, replace = TRUE)
boot_means[i] <- mean(boot_sample)
}
length(boot_mean)
length(boot_means)
hist(boot_means)
inference(ncbirths$gained,type="ci", method="simulation",conflevel=0.9,est="mean",boot_method="perc")
source("http://bit.ly/dasi_inference")
inference(ncbirths$gained,type="ci", method="simulation",conflevel=0.9,est="mean",boot_method="perc")
inference(ncbirths$gained,type="ci", method="simulation",conflevel=0.95,est="median",boot_method="perc")
inference(ncbirths$gained,type="ci", method="simulation",conflevel=0.95,est="median",boot_method="se")
boxplot(ncbirths$weight ~ ncbirths$habit)
boxplot(ncbirths$weight ~ ncbirths$habit, col="rainbow(2)")
boxplot(ncbirths$weight ~ ncbirths$habit, color="rainbow(2)")
boxplot(ncbirths$weight ~ ncbirths$habit, col = c("red", "blue")
)
boxplot(ncbirths$weight ~ ncbirths$habit, col = c("blue", "orange")
)
by(ncbirths$weight, ncbirths$height, mean)
by(ncbirths$weight, ncbirths$habit, mean)
?by
sapply(ncbirths$weight, ncbirths$habit, mean)
ncbirths$habit
ncbirths$weight
sapply(ncbirths$weight, ncbirths$habit, summary)
by(ncbirths$weight, ncbirths$habit, mean)
by(ncbirths$weight, ncbirths$habit, summary)
by(ncbirths$weight, ncbirths$habit, mean())
by(ncbirths$weight, ncbirths$habit)
by(ncbirths$weight, ncbirths$habit, median)
by(ncbirths$weight, ncbirths$habit, mean)
by(ncbirths$weight, ncbirths$habit, summary)
sapply(ncbirths$weight, ncbirths$habit)
?sapply
sapply(ncbirths$weight, mean)
sapply(ncbirths$weight, ncbirths$habit, summary)
?tapply
tapply(ncbirths$weight, ncbirths$habit, summary)
tapply(ncbirths$weight, ncbirths$habit, mean)
tapply(ncbirths$weight, ncbirths$habit, length)
inference(y = ncbirths$weight, x = ncbirths$habit, est = "mean", type = "ci", null = 0, alternative = "twosided", method = "theoretical", order = c("smoker", "nonsmoker"))
inference(y = ncbirths$weight, x = ncbirths$habit, est = "mean", type = "ci", null = 0, alternative = "twosided", method = "theoretical", order = c("smoker", "nonsmoker"))
inference(y = ncbirths$weight, x = ncbirths$habit, est = "media", type = "ci", null = 0, alternative = "twosided", method = "theoretical", order = c("smoker", "nonsmoker"))
inference(y = ncbirths$weight, x = ncbirths$habit, est = "median", type = "ci", null = 0, alternative = "twosided", method = "theoretical", order = c("smoker", "nonsmoker"))
inference(y = ncbirths$weight, x = ncbirths$habit, est = "mean", type = "ci", null = 0, alternative = "twosided", method = "theoretical", order = c("smoker", "nonsmoker"))
inference(y = ncbirths$weight, x = ncbirths$habit, est = "mean", type = "ht", null = 0, alternative = "twosided", method = "theoretical", order = c("smoker", "nonsmoker"))
mean(c(1,2,3))
inference(ncbirths$gained,type="ci", method="simulation",conflevel=0.9,est="mean",boot_method="perc")
inference(y = ncbirths$weight, x = ncbirths$habit, est = "mean", type = "ht", null = 0, alternative = "twosided", method = "simulation", order = c("smoker", "nonsmoker"))
inference(y = ncbirths$weight, x = ncbirths$habit, est = "mean", type = "ci", null = 0, alternative = "twosided", method = "simulation")
tapply(ncbirths$weight, ncbirths$habit, sd)
tapply(ncbirths$weight, ncbirths$habit, mean) + c(-1,1)*tapply(ncbirths$weight, ncbirths$habit, sd)
tapply(ncbirths$weight, ncbirths$habit, summary)
tapply(ncbirths$weight, ncbirths$habit, length)
tapply(ncbirths$weight, ncbirths$habit, mean) + c(-1,1)*tapply(ncbirths$weight, ncbirths$habit, sd)/tapply(ncbirths$weight, ncbirths$habit, length)
smoker <- ncbirths[ncbirths$habit=="smoker",]
Nonsmoker <- ncbirths[ncbirths$habit=="nonsmoker",]
mean(smoker$weight)
mean(smoker$weight, na.rm=T)
mean(Nonsmoker$weight, na.rm=T)
mean(Nonsmoker$weight, na.rm=T) - mean(smoker$weight, na.rm=T)
meanDiff <- mean(Nonsmoker$weight, na.rm=T) - mean(smoker$weight, na.rm=T)
sd(Nonsmoker$weight, na.rm=T)
sd(smoker$weight, na.rm=T)
var(smoker$weight, na.rm=T)
seDiff <- sqrt(var(smoker$weight, na.rm=T)/nrow(smoker) + var(Nonsmoker$weight, na.rm=T)/nrow(Nonsmoker))
seDiff
meanDiff + c(-1,1)*qnorm(0.975)*seDiff
smoker <- ncbirths[ncbirths$habit=="smoker",]
Nonsmoker <- ncbirths[ncbirths$habit=="nonsmoker",]
meanDiff <- mean(Nonsmoker$weight, na.rm=T) - mean(smoker$weight, na.rm=T)
seDiff <- sqrt(var(smoker$weight, na.rm=T)/nrow(smoker) + var(Nonsmoker$weight, na.rm=T)/nrow(Nonsmoker))
meanDiff + c(-1,1)*qnorm(0.975)*seDiff
ncbirths$mage
min(ncbirths$mage)
max(ncbirths$mage)
sorrt(ncbirths$mage)
sort(ncbirths$mage)
hist(ncbirths$mage)
par(mfrow=c(1,1))
boxplot(ncbirths$mage)
summary(ncbirths$mage)
load(url("http://bit.ly/dasi_gss_ws_cl"))
head(gss)
levels(gss$class)
inference(y= gss$wordsum,x = gss$class,est="mean",type="ht",alternative="greater",method="theoretical")
levels(gss$class)
middle=gss[gss$class=="MIDDLE"]
middle=gss[gss$class=="MIDDLE",]
lower=gss[gss$class=="LOWER",]
upper=gss[gss$class=="UPPER",]
working=gss[gss$class=="WORKING",]
aov(gss$class)
anova(gss$class)
names(gss)
aov(gss$class ~ gss$wordsum)
aov(gss$wordsum ~ gss$class)
anova(gss$wordsum ~ gss$class)
aov(gss$wordsum ~ gss$class)
tapply(gss$wordsum, gss$class, mean)
oneway.test(gss$wordsum ~ gss$class)
aov.out <- aov(gss$wordsum ~ gss$class)
summary(aov.out)
TukeyHSD(aov.out)
ncbirts$mature
ncbirths$mature
youngMom <- ncbirths[ncbirths$mature == "younger mom",]
youngMom$mature
max(youngMom$mature)
max(youngMom$mage)
summary(ncbirths$mature)
ucscDb <- dbConnect(MySQL(), user="genome", host = "genome-mysql.cse.ucsc.edu")
install.packages("dbConnect")
library(dbConnect)
install.packages("RMySQL")
install.packages("dbConnect")
library(RMySQL)
require(RMySQL)
install.packages("RMySQL")
getwd()
install.packages("RMySQL")
install.packages("swirl")
install.packages("installr")
updateR()
install.packages("RMySQL")
library(RMySQL)
install.packages("httpuv")
library(httpuv)
setwd("/home/tarek/Analytics/Weeks/Week6-Clustering/Data")
claims <- read.csv("reimbursement.csv")
set.seed(144)
spl = sample(1:nrow(claims), size=0.7*nrow(claims))
train = claims[spl,]
test = claims[-spl,]
train.limited = train
train.limited$reimbursement2009 = NULL
test.limited = test
test.limited$reimbursement2009 = NULL
library(caret)
preproc = preProcess(train.limited)
train.norm = predict(preproc, train.limited)
test.norm = predict(preproc, test.limited)
k = 3
set.seed(144)
km <- kmeans(train.norm, centers = k)
kmClust <- km$cluster
table(kmClust)
library(flexclust)
km.kcca = as.kcca(km, train.norm)
cluster.train = predict(km.kcca)
cluster.test = predict(km.kcca, newdata=test.norm)
sum(cluster.test == 2)
train1 <- subset(train, cluster.train==1)
train2 <- subset(train, cluster.train==2)
train3 <- subset(train, cluster.train==3)
tapply(train1$reimbursement2009, mean)
mean(train1$reimbursement2009)
mean(train1$reimbursement2009); mean(train2$reimbursement2009); mean(train3$reimbursement2009)
MITx: 15.071x The Analytics Edge - PREDICTING MEDICAL COSTS WITH CLUSTER-THEN-PREDICT
========================================================
# Name: Tarek Dib
# Date: 04/22/2014
## *Introduction*
In the second lecture sequence this week, we heard about cluster-then-predict, a methodology in which you first cluster observations and then build cluster-specific prediction models. In the lecture sequence, we saw how this methodology helped improve the prediction of heart attack risk. In this assignment, we'll use cluster-then-predict to predict future medical costs using medical claims data.
In Week 4, we discussed the importance of high-quality predictions of future medical costs based on information available in medical claims data. In this problem, you will predict future medical claims using part of the DE-SynPUF dataset, published by the United States Centers for Medicare and Medicaid Services (CMS). This dataset, available in reimbursement.csv, is structured to represent a sample of patients in the Medicare program, which provides health insurance to Americans aged 65 and older as well as some younger people with certain medical conditions. To protect the privacy of patients represented in this publicly available dataset, CMS performs a number of steps to anonymize the data, so we would need to re-train the models we develop in this problem on de-anonymized data if we wanted to apply our models in the real world.
The observations in the dataset represent a 1% random sample of Medicare beneficiaries in 2008, limited to those still alive at the end of 2008. The dependent variable, reimbursement2009, represents the total value of all Medicare reimbursements for a patient in 2009, which is the cost of the patient's care to the Medicare system. The following independent variables are available:
age: The patient's age in years at the beginning of 2009
alzheimers: Binary variable for whether the patient had diagnosis codes for Alzheimer's       disease or a related disorder in 2008
arthritis: Binary variable for whether the patient had diagnosis codes for rheumatoid arthritis or osteoarthritis in 2008
cancer: Binary variable for whether the patient had diagnosis codes for cancer in 2008
copd: Binary variable for whether the patient had diagnosis codes for Chronic Obstructive Pulmonary Disease (COPD) in 2008
depression: Binary variable for whether the patient had diagnosis codes for depression in 2008
diabetes: Binary variable for whether the patient had diagnosis codes for diabetes in 2008
heart.failure: Binary variable for whether the patient had diagnosis codes for heart failure in 2008
ihd: Binary variable for whether the patient had diagnosis codes for ischemic heart disease (IHD) in 2008
kidney: Binary variable for whether the patient had diagnosis codes for chronic kidney disease in 2008
osteoporosis: Binary variable for whether the patient had diagnosis codes for osteoporosis in 2008
stroke: Binary variable for whether the patient had diagnosis codes for a stroke/transient ischemic attack (TIA) in 2008
reimbursement2008: The total amount of Medicare reimbursements for this patient for 2008
## *PREPARING THE DATASET*
```{r}
# Data
setwd("/home/tarek/Analytics/Weeks/Week6-Clustering/Data")
claims <- read.csv("reimbursement.csv")
# Subset of claims. Patients who had at least one of the chronic conditions
has.condition = subset(claims, alzheimers == 1 | arthritis == 1 | cancer == 1 | copd == 1 | depression == 1 | diabetes == 1 | heart.failure == 1 | ihd == 1 | kidney == 1 | osteoporosis == 1 | stroke == 1)
# Ratio
nrow(has.condition) / nrow(claims)
# Maximum correlation between independent variables
sort(cor(claims[-(13:14)]))[132]
# Distribution of reimbursement in 2008
hist(claims$reimbursement2008)
# Transform the dependent variables. +1 so that we don't get log(0) which is -infinity
claims$reimbursement2008 = log(claims$reimbursement2008+1)
claims$reimbursement2009 = log(claims$reimbursement2009+1)
hist(claims$reimbursement2009, main="Log of Reimbursement in 2009")
# About 20% of beneficiaries had $0 reimbursement in 2009
sum(claims$reimbursement2009==0) / length(claims$reimbursement2009)
```
## *INITIAL LINEAR REGRESSION MODEL*
```{r}
set.seed(144)
spl = sample(1:nrow(claims), size=0.7*nrow(claims))
train = claims[spl,]
test = claims[-spl,]
lm.claims <- lm(reimbursement2009~., data=train)
summary(lm.claims)
# MSE
predTest <- predict(lm.claims, newdata=test)
SSE <- sum((predTest - test$reimbursement2009)^2)
rms.lm <- sqrt(SSE/nrow(test))
rms.lm
# The naive baseline predicts the average of the dependent variable (reimbursement2009) on the training set! baseline model is mean(train$reimbursement2009). The testing MSE is:
rms.baseline <- sqrt(mean((mean(train$reimbursement2009) - test$reimbursement2009)^2))
rms.baseline
# Smart baseline model: predict that a patient's medical costs would be equal to their costs in the previous year
smartModel <- train$reimbursement2008
# RMSE
sqrt(mean((test$reimbursement2008-test$reimbursement2009)^2))
```
## *CLUSTERING MEDICARE BENEFICIARIES*
```{r}
# remove the dependent variable using the following commands:
train.limited = train
train.limited$reimbursement2009 = NULL
test.limited = test
test.limited$reimbursement2009 = NULL
# In cluster-then-predict, our final goal is to predict the dependent variable, which is unknown to us at the time of prediction. Therefore, if we need to know the outcome value to perform the clustering, the methodology is no longer useful for prediction of an unknown outcome value.
#This is an important point that is sometimes mistakenly overlooked. If you use the outcome value to cluster, you might conclude your method strongly outperforms a non-clustering alternative. However, this is because it is using the outcome to determine the clusters, which is not valid.
# Normalize
library(caret)
preproc = preProcess(train.limited)
train.norm = predict(preproc, train.limited)
test.norm = predict(preproc, test.limited)
mean(train.norm$arthritis)
```
## *K-means Clustering*
```{r}
k = 3
set.seed(144)
km <- kmeans(train.norm, centers = k)
kmClust <- km$cluster
table(kmClust)
km$centers
# Use the flexclust package to obtain training set and testing set cluster assignments for our observations
library(flexclust)
km.kcca = as.kcca(km, train.norm)
cluster.train = predict(km.kcca)
cluster.test = predict(km.kcca, newdata=test.norm)
sum(cluster.test == 2)
train1 <- subset(train, cluster.train==1)
train2 <- subset(train, cluster.train==2)
train3 <- subset(train, cluster.train==3)
mean(train1$reimbursement2009); mean(train2$reimbursement2009); mean(train3$reimbursement2009)
lm1 <- lm(reimbursement2009~., data=train1)
summary(lm1)
lm2 <- lm(reimbursement2009~., data=train2)
summary(lm2)
lm3 <- lm(reimbursement2009~., data=train3)
summary(lm3)
summary(lm1); summary(lm2); summary(lm3)
test1 = subset(test, cluster.test == 1)
test2 = subset(test, cluster.test == 2)
test3 = subset(test, cluster.test == 3)
pred.Test1 <- predict(lm1, newdata=test1)
pred.Test2 <- predict(lm2, newdata=test2)
pred.Test3 <- predict(lm3, newdata=test3)
c(mean(pred.Test1), mean(pred.Test2), mean(pred.Test3))
names(a) = c("pred.Test1", "pred.Test2", "pred.Test3")
a <- c(mean(pred.Test1), mean(pred.Test2), mean(pred.Test3))
names(a) = c("pred.Test1", "pred.Test2", "pred.Test3")
a
pred.TestMeans <- c(mean(pred.Test1), mean(pred.Test2), mean(pred.Test3))
names(pred.TestMeans) = c("pred.Test1", "pred.Test2", "pred.Test3")
pred.TestMeans
which.max(pred.TestMeans)
which.min(pred.TestMeans)
RMSE.Test1 <- sqrt(mean(pred.Test1 - test1$reimbursement2009)^2)
RMSE.Test1
RMSE.Test <- c(RMSE.Test1, RMSE.Test2, RMSE.Test3)
RMSE.Test1 <- sqrt(mean(pred.Test1 - test1$reimbursement2009)^2)
RMSE.Test2 <- sqrt(mean(pred.Test2 - test2$reimbursement2009)^2)
RMSE.Test3 <- sqrt(mean(pred.Test3 - test3$reimbursement2009)^2)
RMSE.Test <- c(RMSE.Test1, RMSE.Test2, RMSE.Test3)
RMSE.Test
names(RMSE.Test) = c(RMSE.Test1, RMSE.Test2, RMSE.Test3)
which.max(RMSE.Test)
all.predictions = c(pred.Test1, pred.Test2, pred.Test3)
all.outcomes = c(test1$reimbursement2009, test2$reimbursement2009, test3$reimbursement2009)
sqrt(mean(all.predictions - all.outcomes)^2)
length(all.predictions)
length(all.outcomes)
SSE.all <- sum((all.predictions - all.outcomes)^2)
rms.all <- sqrt(SSE.all/length(all.outcomes))
rms.all
sqrt(mean((all.predictions - all.outcomes)^2))
RMSE.Test1 <- sqrt(mean((pred.Test1 - test1$reimbursement2009)^2))
RMSE.Test2 <- sqrt(mean((pred.Test2 - test2$reimbursement2009)^2))
RMSE.Test3 <- sqrt(mean((pred.Test3 - test3$reimbursement2009)^2))
RMSE.Test <- c(RMSE.Test1, RMSE.Test2, RMSE.Test3)
names(RMSE.Test) = c(RMSE.Test1, RMSE.Test2, RMSE.Test3)
which.max(RMSE.Test)
rms.lm
