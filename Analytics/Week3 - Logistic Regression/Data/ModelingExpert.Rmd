15.071 - Analytics Edge
------------------------------------------------------------------
#### Claims Data
#### Tarek Dib
#### Date: March 22, 2014

### *Introduction*
Data of 131 Diabetes patients were selected randomly from the D2Hawkeye database. The patients range in age from 35 to 55 years. The costs range from $10,000 to $20,000. The dates of these claims range from September 1, 2003 to August 31, 2005.

Expert physician reviewed claims and wrote descriptive notes, and rated quality on a two-point
scale (poor/good). Dependent variable is quality of care. Independent variables are listed below in addidition to the binary dependent variable PoorCare (0 if the patient had good care and 1 if he/she had poor care).

The variables in the dataset quality.csv are as follows:

    MemberID numbers the patients from 1 to 131, and is just an identifying number.
    InpatientDays is the number of inpatient visits, or number of days the person spent in the hospital.
    ERVisits is the number of times the patient visited the emergency room.
    OfficeVisits is the number of times the patient visited any doctor's office.
    Narcotics is the number of prescriptions the patient had for narcotics.
    DaysSinceLastERVisit is the number of days between the patient's last emergency room visit and the end of the study period (set to the length of the study period if they never visited the ER). 
    Pain is the number of visits for which the patient complained about pain.
    TotalVisits is the total number of times the patient visited any healthcare provider.
    ProviderCount is the number of providers that served the patient.
    MedicalClaims is the number of days on which the patient had a medical claim.
    ClaimLines is the total number of medical claims.
    StartedOnCombination is whether or not the patient was started on a combination of drugs to treat their diabetes (TRUE or FALSE).
    AcuteDrugGapSmall is the fraction of acute drugs that were refilled quickly after the prescription ran out.
    PoorCare is the outcome or dependent variable, and is equal to 1 if the patient had poor care, and equal to 0 if the patient had good care.

## *Summary and Classification*
```{r}
quality = read.csv("quality.csv")
# Look at structure
str(quality)

# Table outcome
table(quality$PoorCare)
# Percentage of patients who receive good quality care
table(quality$PoorCare)[1]/nrow(quality)   # ~75%

# Load the classification library caTools   
library(caTools)

# Split the data into training and test sets
set.seed(888)
spl = sample(1:nrow(quality), size=0.7*nrow(quality))
train = quality[spl,]
test = quality[-spl,]

# Randomly split data
set.seed(88)
# 75% of patients in the training set are receiving good care, and 75% of patients in the test set are receing good care 
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split   # True means that we should put that observation in the training set. False means we should put it in the test set.
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
```

## *Building the Model*
```{r}
# Logistic Regression Model
QualityLog = glm(PoorCare ~ StartedOnCombination + ProviderCount, data=qualityTrain, family=binomial)
summary(QualityLog)
# All else being equal, the above model implies that starting a patient on a combination of drugs is indicative of poor care because the coefficient value is positive, meaning that positive values of the variable make the outcome of 1 more likely.

# Make predictions on training set
predictTrain = predict(QualityLog, type="response")

# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
```

## *Confusion Matrix*
```{r}
# Confusion matrix for threshold of 0.5
table(qualityTrain$PoorCare, predictTrain > 0.5)

# Sensitivity and Specificity
Sensitivity <- table(qualityTrain$PoorCare, predictTrain > 0.5)[2,2]/(table(qualityTrain$PoorCare, predictTrain > 0.5)[2,1] + table(qualityTrain$PoorCare, predictTrain > 0.5)[2,2])
Specificity <- table(qualityTrain$PoorCare, predictTrain > 0.5)[1,1]/(table(qualityTrain$PoorCare, predictTrain > 0.5)[1,1] + table(qualityTrain$PoorCare, predictTrain > 0.5)[1,2])
Sensitivity; Specificity

# Confusion matrix for threshold of 0.7
table(qualityTrain$PoorCare, predictTrain > 0.7)
```

## *Receiver Operating Characteristic Curve (ROC Curve)*
```{r}
library(ROCR)

# Prediction function
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)

# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
```

## *ROC Curve*
```{r fig.width=14, fig.height=8}
# ROC Curve with threshold labels and added colors 
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.2), text.adj=c(-0.2,1.5))
```

## *Predicting using the test set*
```{r}
predictTest = predict(QualityLog, type="response", newdata=qualityTest)
# compute the test set AUC
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
```
The AUC of a model has the following nice interpretation: given a random patient who actually received poor care, and a random patient who actually received good care, the AUC is the perecentage of time that our model will classify which is which correctly. 