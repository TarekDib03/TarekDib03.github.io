logdata <- log2(expdata)
boxplot(as.data.frame(logdata))
boxplot(as.data.frame(logdata))
class(logdata)
probemeans <- apply(logdata, 1, mean)
probesd <- apply(logdata, 1, sd)
plot(probemeans, probesd)
q25 <- quantile(probemeans, 0.25)
whichtosave <- which(probemeans > q25)
q25logdata <- logdata[whichtosave,]
mydata <- q25logdata[apply(q25logdata, 1, IQR) > 1.5, ]
dim(mydata)[1]
tdata <- aperm(mydata)
nrow(tdata)
ncol(tdata)
tdata <- aperm(mydata, c(2,1))
ncol(tdata)
nrow(tdata)
tdata <- t(mydata)
nrow(tdata)
tdata <- aperm(mydata, 1:2)
nrow(tdata)
ncol(tdata)
tdata <- aperm(mydata, c(1,2))
ncol(tdata)
tdata <- t(mydata)
pca <- prcomp(tdata, scale=T)
summary(pca)
plot(pca$x, type="n")
text(pca$x, rownames(pca$x), cex=0.5)
show(eset)
conditions <- phenoData(eset)$agent
plot(pca$x, type="n")
text(pca$x, labels=conditions, cex=0.5)
summary(pca)
pearsonCorr <- as.dist(1 - cor(mydata))
hC <- hclust(pearsonCorr)
plot(hC, labels = sampleNames(eset))
?as.dist
plot(hC, labels = conditions)
heatmap(mydata, col=greenred(100))
par(mar=c(5.1,4.1,4.1,2.1))
heatmap(mydata, col=greenred(100))
par(mar=c(3,4.1,4.1,2.1))
heatmap(mydata, col=greenred(100))
par(mar=c(7,4.1,4.1,2.1))
heatmap(mydata, col=greenred(100))
par(mar=c())
heatmap(mydata, col=greenred(100))
par(mai=c(1.02,0.82,0.82,0.42))
heatmap(mydata, col=greenred(100))
heatmap(mydata, col=greenred(100), cex=0.5)
heatmap(mydata, col=greenred(10))
heatmap(mydata)
heatmap(mydata, col=greenred(100))
heatmap(mydata, col=greenred(100), las=1)
heatmap(mydata, col=greenred(100), las=2)
heatmap(mydata, col=greenred(100))
par(oma=c(4,4,4,4)
)
heatmap(mydata, col=greenred(100))
par(oma=c(3,4,4,4)
)
heatmap(mydata, col=greenred(100))
par(oma=c(3,3,3,3))
heatmap(mydata, col=greenred(100))
par(oma=c(3,2,2,2))
heatmap(mydata, col=greenred(100))
par(oma=c(2,2,2,2))
heatmap(mydata, col=greenred(100))
par(oma=c(2,0,0,0))
heatmap(mydata, col=greenred(100))
par(oma = c(1.5, 0, 0, 0))
heatmap(mydata, col=greenred(100))
design <- model.matrix(~0+condfactor)
condfactor <- factor(eset$agent)
# Construct a model matrix, and assign the names to the columns:
design <- model.matrix(~0+condfactor)
colnames(design) <- c("ctrl", "tnf")
design
fit <- lmFit(eset, design)
fit
summary(fit)
contrastmatrix <- makeContrasts(tnf - ctrl,levels=design)
fit <- contrasts.fit(fit, contrastmatrix)
ebayes <- eBayes(fit)
show(ebays)
show(ebayes)
hist(ebayes$p.value)
results <- decideTests(ebayes)
length(results != 0)
sum(results != 0)
sum(results = 1 | results= -1)
sum(results == 1 | results == -1)
sum(results == T)
sum(results == TRUE)
length(which(results != 0))
?decideTests
vennDiagram(results)
resData <- exprs(eset)[results != 0,]
geneSymbol <- as.array(fData(eset)[,"Gene symbol"])
gs <- geneSymbol[c(which(results != 0))]
rownames(resData) <- gs
pvalues <- ebayes$p.value[results != 0,]
resData <- cbind(resData, pvalues)
# And add-p values corrected for multiple testing (q-values):
adj.pvalues <- p.adjust(ebayes$p.value, method="BH")
adj.pvalues <- adj.pvalues[results != 0]
resData <- cbind(resData, adj.pvalues)
write.table(resData, "most_regulated.txt", sep="\t")
getwd()
?write.table
baseline <- c(140, 138, 150, 148, 135)
twoWeeks <- c(132, 135, 151, 146, 130)
t.test(baseline, twoWeeks, two.sided = T, paired = T)
t.test(baseline, twoWeeks, two.sided = T, paired = T)$p-value
t.test(baseline, twoWeeks, two.sided = T, paired = T)$p.value
?qt
n <- 9
s <- 30
x.bar <- 1100
x.bar + c(-1,1)*qt(0.975, n - 1)*s
?pbinom
pbinom(3:4, 4, 0.75)
dbinom(3:4, 4, 0.75)
pbinom(3, 4, 0.75)
pbinom(4, 4, 0.75)
0.75^4
4choose3
4choose(3)
choose(4,3)
choose(4,3)*0.75^3*0.25 + 0.75^4
1 - (choose(4,3)*0.75^3*0.25 + 0.75^4)
dbinom(3:4, 4, 0.75)
pbinom(3, 4, 0.75)
1 - pbinom(3, 4, 0.75)
prop.test(3, 4, alternative = "greater")
prop.test(3, 4, alternative = "greater")$p.value
binom.test(3, 4, alternative = "greater")$p.value
binom.test(3, 4, alternative = "greater")
binom.test(3, 4, alternative = "less")
binom.test(3, 4, alternative = "greater")
?prop.test
prop.test(10, 1787, 0.01, alternative = "less")
prop.test(10, 1787, 0.01, alternative = "less")$p.value
10/1787
p <- 1/100; P <- 10/1787
n <- 1787
sigma <- sqrt(p*(1 - p)/n)
z <- (p - P) / sigma
p_value <- pnorm(z)
p_value
p_value <- 1 - pnorm(z)
p_value
prop.test(10, 1787, 0.01, alternative = "greater")$p.value
prop.test(10, 1787, 0.01, alternative = "less")$p.value
prop.test(1, 1787, 0.01, alternative = "less")$p.value
prop.test(10, 1787, 1/100, alternative = "less")$p.value
sigma1 <- 1.8; sigma2 <- 1.5
n1 <- 9; n2 <- 9
df <- n1 + n2 - 2
sigma.pooled <- sqrt((sigma1^2 + sigma2^2)/2)
SE <- sigma.pooled*sqrt(1/n1 + 1/n2)
Diff1 <- -3; Diff2 <- 1
t <- (Diff1 - Diff2) / SE
p.value <- pt(t, df)
p.value
cl <- 0.9
lower <- 1077
upper <- 1123
sl <- 0.05
n <- 9
df <- n - 1
# The confidence interval is given as x.bar + c(-1,1)*t*SE.
# lower = x.bar - t*SE. upper = x.bar + t*SE. x.bar = lower + t*SE and
# x.bar = upper - t*SE. Thus, lower + t*SE = upper - t*SE. Therefore, SE = (upper - lower)/t
t <- qt(1 - 0.1/2, df)
t
SE <- (upper - lower) / t
SE
SE <- (upper - lower) / (2*t)
SE
x.bar <- 1100
mu <- 1078
t2 <- (x.bar - mu) / SE
pt(t2, df)
1 - pt(t2, df)
1 - pt(t2, 8)
t2
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, alternative = "two.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, alternative = "one.sided")
power.t.test(n = NULL, delta = 0.01, sd = 0.04, sig.level = 0.05, alternative = "one.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, power = 0.9, alternative = "one.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, power = 0.9, alternative = "one.sided")
?power.t.test
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, power = 0.9, type = "paired", alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, power = 0.9, type = "paired", alternative = "one.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, type = "paired", alternative = "one.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, type = "paired", alternative = "one.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.1, type = "paired", alternative = "one.sided")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.15, type = "paired", alternative = "one.sided")
xbar1 <- 44; xbar2 <- 42.04
s <- 12
n <- 288
z <- (xbar1 - xbar2) /(s/sqrt(n))
z
1 - pnorm(z)
2*(1 - pnorm(z))
1 - 2*pnorm(z)
12/sqrt(n)
2*(1 - pnorm(z))
se <- sqrt(2/n)*s
z <- (xbar1 - xbar2) / se
2*(1 - pnorm(z))
n <- 9
s <- 30
x.bar <- 1100
x.bar + c(-1,1)*qt(0.975, n - 1)*s/sqrt(n)
set.seed(123)
lambda <- 0.2
n <- 40
num_sim <- 1000
# execute simulation
data <- data.frame(mean = numeric(num_sim), sd = numeric(num_sim))
for( i in 1:num_sim) {
sim <- rexp(n, lambda)
data[i,"mean"] <- mean(sim)
data[i,"sd"]   <- sd(sim)
}
# show initial lines of results
head(data)
var(mean)
var(data$mean)
set.seed(123)
lambda <- 0.2
n <- 40
num_sim <- 10000
# execute simulation
data <- data.frame(mean = numeric(num_sim), sd = numeric(num_sim))
for( i in 1:num_sim) {
sim <- rexp(n, lambda)
data[i,"mean"] <- mean(sim)
data[i,"sd"]   <- sd(sim)
}
# show initial lines of results
head(data)
var(data$mean)
mean_1 <- mean(data$mean)
var_1  <- var(data$mean)
# theoretical
mean_0 <- 1/lambda
var_0  <- ((1/lambda)^2)/n
mean_1
var_1
mean_0
var_0
library(ggplot2)
# plot the mean data
g <- ggplot(data = data, aes(x = mean)) +
geom_histogram(aes(y=..density..), binwidth = 0.2,alpha = 0.3, fill = "blue", color="black" )
g <- g + stat_function(fun = dnorm, arg = list(mean = mean_1, sd = sqrt(var_1)),color="blue")
g <- g + geom_vline(xintercept = mean_1, color = "blue",  linetype="dashed", size=1)
g <- ggplot(data = data, aes(x = mean)) +
geom_histogram(aes(y=..density..), binwidth = 0.2,alpha = 0.3, fill = "blue", color="black" ) +
stat_function(fun = dnorm, arg = list(mean = mean_1, sd = sqrt(var_1)),color="blue") +
geom_vline(xintercept = mean_1, color = "blue",  linetype="dashed", size=1)
g
ggplot(data = data, aes(x = mean)) +
geom_histogram(aes(y=..density..), binwidth = 0.2,alpha = 0.3, fill = "blue", color="black" ) +
stat_function(fun = dnorm, arg = list(mean = mean_1, sd = sqrt(var_1)),color="blue") +
geom_vline(xintercept = mean_1, color = "blue",  linetype="dashed", size=1.5)
ggplot(data = data, aes(x = mean)) +
geom_histogram(aes(y=..density..), binwidth = 0.2,alpha = 0.3, fill = "blue", color="black" ) +
stat_function(fun = dnorm, arg = list(mean = mean_1, sd = sqrt(var_1)),color="blue") +
geom_vline(xintercept = mean_1, color = "red",  linetype="dashed", size=1.5)
# Normalize the means
data$normalized <- (data$mean - 1/lambda)/(1/lambda * 1/sqrt(n))
g <- qplot(data$normalized, geom = "blank")
g <- g + geom_line(aes(y = ..density.., color = "Estimated"), stat = "density")
g <- g + stat_function(fun = dnorm, aes(color = "Normal"))
g
g <- qplot(data$normalized, geom = "blank")
g <- g + geom_line(aes(y = ..density.., color = "Estimated"), stat = "density")
g <- g + stat_function(fun = dnorm, aes(color = "Normal")) + xlab("Normalized Means")
g
data$normalized <- (data$mean - 1/lambda)/(1/lambda * 1/sqrt(n))
g <- qplot(data$normalized, geom = "blank")
g <- g + geom_line(aes(y = ..density.., color = "Estimated"), stat = "density")
g <- g + stat_function(fun = dnorm, aes(color = "Normal")) + xlab("Normalized Means")
g + scale_fill_discrete(name="")
g + guides(fill=guide_legend(title="New Legend Title"))
g <- g + guides(fill=guide_legend(title="New Legend Title"))
g
g <- g + guides(fill=guide_legend(title="New Legend Title")) +
scale_fill_manual(legend_title,values=c("orange","red"))
g <- g + guides(fill=guide_legend(title="New Legend Title")) +
scale_fill_manual("",values=c("orange","red"))
g
g <- qplot(data$normalized, geom = "blank")
g <- g + geom_line(aes(y = ..density.., color = "Estimated"), stat = "density")
g <- g + stat_function(fun = dnorm, aes(color = "Normal")) + xlab("Normalized Means")
g <- g + guides(fill=guide_legend(title="New Legend Title")) +
scale_fill_manual("hello",values=c("orange","red"))
g
g <- qplot(data$normalized, geom = "blank")
g <- g + geom_line(aes(y = ..density.., color = "Estimated"), stat = "density")
g <- g + stat_function(fun = dnorm, aes(color = "Normal")) + xlab("Normalized Means")
g + guides(color=guide_legend(title="sale year"))
g + guides(color=guide_legend(title=""))
names(data)
n <- 40;
lvals <- seq(.1, .9, by = .05);
nosim <- 1000
coverage <- sapply(lvals, function(p){
phatsv <- matrix(rexp(n*nosim, p),nosim)
phats <- apply(phatsv,1,mean)
sdphat <- apply(phatsv,1,sd)
ll <- 1/phats - qnorm(.975) * (1/sdphat) /sqrt(n)
ul <- 1/phats + qnorm(.975) * (1/sdphat) /sqrt(n)
mean(ll < p & ul > p)
})
g <- ggplot(data.frame(lvals, coverage), aes(x = lvals, y = coverage)) +
geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)
g
library(datasets)
library(xtable)
data(ToothGrowth)
tab <- xtable(summary(ToothGrowth))
print(tab, type='html')
library(xtable)
data(tli)
## Demonstrate data.frame
tli.table <- xtable(tli[1:10,])
digits(tli.table)[c(2,6)] <- 0
print(tli.table,floating=FALSE)
install.packages("manipulate")
install.packages(c("DEoptimR", "highr", "httpuv", "jsonlite", "RcppArmadillo", "RSQLite"))
install.packages("manipulate")
library("manipulate")
knit_hooks$set(plot = knitr:::hook_plot_html)
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
if(is.numeric(x)) {
round(x, getOption('digits'))
} else {
paste(as.character(x), collapse = ', ')
}
})
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library(UsingR)
data(galton)
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
hist(galton$child,col="blue",breaks=100)
meanChild <- mean(galton$child)
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
meanChild
table(galton$child, galton$parent)
as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent", ylab = "child")
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent", ylab = "child")
library("ggplot2")
library("manipulate")
chromium <- read.csv("chromium.csv")
nickel <- read.csv("nickel.csv")
manipulate({
p<- ggplot(data, aes(air, bm)) + geom_point()
p+facet_grid(facet)+scale_x_continuous(trans=xScale)+scale_y_continuous(trans=yScale)
},
yScale=picker("Linear"="identity","Log"="log10",label="Y Scale Transformation"),
xScale=picker("Linear"="identity","Log"="log10",label="X Scale Transformation"),
facet=picker( "None" = ". ~ .", "RPE" = ". ~ rpe","Welding type" = ". ~ welding.type","RPE and Welding type" = "rpe ~ welding.type",initial="None",label="Faceting"),
data=picker("Chromium"=chromium,"Nickel"=nickel,label="Datasets")
)
library("ggplot2")
library("manipulate")
chromium <- read.csv("chromium.csv")
$y_i = \alpha + \beta x_i + e_i$
library(knitr)
kable(mtcars, 'html', table.attr='id="mtcars_table"')
install.packages("DataTables")
getwd()
render("test.md")
library(knitr)
render("test.md")
library(rmarkdown)
library(markdown)
render("test.Rmd")
render("test.Rmd", "pdf_document")
system("pdflatex --version")
render("test.Rmd", "pdf_document")
sessionInfo
sessionInfo()
render("test.Rmd", "all")
design.matrix <- model.matrix(~ sex*grade, data=tli[1:10,])
design.table <- xtable(design.matrix)
print(design.table,floating=FALSE)
print(design.table)
print(design.table,floating=FALSE)
fm2 <- lm(tlimth ~ sex * ethnicty, data = tli)
print(xtable(anova(fm2)), type="html")
install.packages("pander")
library(pander)
install.packages("googleVis")
library(Animals)
library(MASS)
data(Animals)
head(Animals)
library(googleVis)
?gvisTable
data(Population)
tbl2 <- gvisTable(Population, options=list(page='enable',
height='automatic',
width='automatic'))
plot(tbl2)
tbl2 <- gvisTable(Population, options=list(page='enable',
height='automatic',
width='automatic'))
plot(tbl2)
print(tbl2)
source('~/tableExamples/style.R')
?lm
data(mtcars)
pairs(mtcars)
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training)
head(dummies)
head(predict(dummies,newdata=training))
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)
lines(trainFaith$waiting,lm1$fitted,lwd=3)
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
summary(modFit$finalModel)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
groups = c(1:3,2:4)
tapply(groups, groups, sum)
?tapply
setwd("IntroductionToDataScience")
?download.file
hdata <- read.table("homes76.txt", header=T, sep="\t")
hdata[1:5, 1:5]
hdata <- read.table("homes76.txt", sep="\t")
hdata[1:5, 1:5]
hdata <- read.table("homes76.txt", header=T)
hdata[1:5, 1:5]
names(hdata)
?!exist
